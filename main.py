"""
    main.py
    -------

    This module contains the main functions to extract comments and relative text from word files and convert them into
    EAI Platform annotations
"""

from doc_converter import get_text_from_doc, normalize_fucked_encoding
import zipfile
from bs4 import BeautifulSoup as Soup
import re
from datetime import datetime
from tqdm import tqdm
from entities import extraction, taxonomy
from typing import Union
from platform_utils_eai.functions import create_folder_structure, create_libraries_zip
from pathlib import Path


def return_comments_dicts(doc_file_path: Path) -> Union[list, bool]:
    """
    In a word file, for every comment in it creates a dict with the text of the comment, the text on which the
    comment was placed, the start character of the comment and the end character of the comment. Then returns either a list
    of dictionaries (one dictionary per comment in the file), or False if no comment was found in the docx.

    :param doc_file_path: path of a single word document (.docx)
    :return: list of dictionaries (one for each comment) or False
    """

    unzip = zipfile.ZipFile(doc_file_path)
    try:
        comments = Soup(unzip.read('word/comments.xml'), 'lxml')
        doc = unzip.read('word/document.xml').decode()
        txt = get_text_from_doc(doc_file_path)
        txt = re.sub('(S\d+)', r'\n\1', txt)  # newline ogni enunciato
        txt = re.sub(' +', ' ', txt)

        #  populate comments_dicts list with dicts for every comment
        start_loc = {x.group(1): x.start() for x in re.finditer(r'<w:commentRangeStart.*?w:id="(.*?)"', doc)}
        end_loc = {x.group(1): x.end() for x in re.finditer(r'<w:commentRangeEnd.*?w:id="(.*?)".*?>', doc)}
        comments_dicts = []
        for c in comments.find_all('w:comment'):
            c_id = c.attrs['w:id']
            # Use the locations we found earlier to extract the xml fragment from the document for
            # each comment ID, adding spaces to separate any paragraphs in multi-paragraph comments
            xml = re.sub(r'(<w:p .*?>)', r'\1 ', doc[start_loc[c_id]:end_loc[c_id] + 1])
            # print(xml+"\n\n")
            cmt = ''.join(c.findAll(text=True))
            cmt = re.sub(' +', ' ', cmt)
            cmt = re.sub('-', '', cmt)
            text_reference_soup = Soup(xml, 'lxml').findAll(text=True)
            # print(text_reference_soup)
            if len(text_reference_soup) > 1:
                text_reference = "".join(text_reference_soup)
            elif len(text_reference_soup) == 1:
                text_reference = text_reference_soup[0]
            else:
                text_reference = "text reference not found"

            text_reference = normalize_fucked_encoding(text_reference)
            text_reference = re.sub(' +', ' ', text_reference)
            text_reference = re.sub('(S\d+)', r'\n\1', text_reference)

            cmt_and_txt = {"comment": re.split(":|,|;", cmt)[0].strip(), "text": text_reference,
                           "start": txt.find(text_reference), "end": txt.find(text_reference) + len(text_reference)}
            # print(cmt_and_txt)

            comments_dicts.append(cmt_and_txt)
        return comments_dicts
    except KeyError:
        print(f"No comments found in {doc_file_path.name}")
        return False


def create_annotation_and_text_file(doc_file_path: Path, comments_dicts: list, folders: dict, highlight_mode: bool = True):
    """
    For each comment in comment_dicts create a corresponding annotation and test file in their relative folders

    :param doc_file_path: path of a single word document (.docx)
    :param comments_dicts: list of comment dictionaries generated by :func:`main.return_comments_dicts`
    :param folders: folder structure generated by :func:`platform_utils_eai.functions.create_folder_structure`
    :param highlight_mode: if true, for every comment in the current file, create an ann file with the annotation for the text highlighted by the comment. This works better for autoML, else create a single ann file for the current file with all the annotations for all the comments
    """
    if highlight_mode:
        for idx, c in enumerate(comments_dicts):
            text_file_name = doc_file_path.name + "_" + str(idx) + ".txt"
            ann_file_name = doc_file_path.name + "_" + str(idx) + ".ann"
            if c['comment'].lower() in taxonomy:
                with open(Path(folders["tax_test_folder"] + "/" + text_file_name), 'w', encoding="utf-8") as file:
                    # print(txt)
                    file.write(c['text'])

                with open(Path(folders["tax_ann_folder"] + "/" + ann_file_name), 'a', encoding="utf-8") as ann:
                    tax_count = 1
                    # print("TAX: ", c["text"])
                    ann.write(f"C{tax_count}		{taxonomy[c['comment'].lower()]}\n")

            if c['comment'].lower() in extraction:
                if c["start"] == -1:
                    # raise Exception(f"'{c['text']}' not found in text ({docfile.split('.')[0]})")
                    pass
                else:
                    with open(Path(folders["xtr_test_folder"] + "/" + text_file_name), 'w',
                              encoding="utf-8") as file:
                        # print(txt)
                        file.write(c['text'])
                    with open(Path(folders["xtr_ann_folder"] + "/" + ann_file_name), 'a',
                              encoding="utf-8") as ann:
                        xtr_count = 1
                        # print("XTR: ", c["text"])
                        ann.write(
                            f"T{xtr_count}		{extraction[c['comment'].lower()]} {c['start']} {c['end']}	{c['text']}\n")

    if not highlight_mode:
        tax_count = 1
        xtr_count = 1

        txt = normalize_fucked_encoding(get_text_from_doc(doc_file_path))
        txt = re.sub(' +', ' ', txt)
        txt = re.sub('(S\d+)', r'\n\1', txt) # newline ogni enunciato

        tax_txt_path = Path(folders["tax_test_folder"] + "/" + str(doc_file_path.with_suffix(".txt").name))
        tax_ann_path = Path(folders["tax_ann_folder"] + "/" + str(doc_file_path.with_suffix(".ann").name))
        xtr_txt_path = Path(folders["xtr_test_folder"] + "/" + str(doc_file_path.with_suffix(".txt").name))
        xtr_ann_path = Path(folders["xtr_ann_folder"] + "/" + str(doc_file_path.with_suffix(".ann").name))

        for c in comments_dicts:
            if c['comment'].lower() in taxonomy.keys():

                # evita di ricreare il file txt tante volte quante sono le annotazioni
                if not tax_txt_path.exists():
                    print(f"{tax_txt_path} not exist, creating it.")
                    with open(tax_txt_path, 'a', encoding="utf-8") as text_file:
                        text_file.write(txt)
                else:
                    print(f"{tax_txt_path} exist")

                with open(tax_ann_path, 'a', encoding="utf-8") as ann:
                    ann.write(f"C{tax_count}		{taxonomy[c['comment'].lower()]}\n")
                    tax_count += 1

            if c['comment'].lower() in extraction.keys():
                if not xtr_txt_path.exists():
                    print(f"{xtr_txt_path} not exist, creating it.")
                    with open(xtr_txt_path, 'a', encoding="utf-8") as text_file:
                        text_file.write(txt)
                else:
                    print(f"{xtr_txt_path} exist")

                with open(xtr_ann_path, 'a', encoding="utf-8") as ann:
                    if c["start"] == -1:
                        # raise Exception(f"'{c['text']}' not found in text ({docfile.split('.')[0]})")
                        pass
                    else:
                        ann.write(
                            f"T{xtr_count}		{extraction[c['comment'].lower()]} {c['start']} {c['end']}	{c['text']}\n")
                        xtr_count += 1
    print(f"Found {len(comments_dicts)} annotations")


def main():
    runs_folder_path = Path(f"{input('input the path where the run output should be stored: ')}")

    if not runs_folder_path.exists():
        raise Exception(f"Path {runs_folder_path} doesn't exist")
    else:
        now = datetime.now().strftime('%d_%m_%y_%H_%M')
        folders = create_folder_structure(str(runs_folder_path), now)

        word_files_path = Path(r"C:\Users\smarotta\Desktop\mirco_trasc_ann\word_sample")
        for f in tqdm(word_files_path.glob("*.docx")):
            print("---------\n" + "WORKING ON: ", f.name)
            if "_annotato" in f.name:
                file_comments_dicts = return_comments_dicts(f)
                create_annotation_and_text_file(f, file_comments_dicts, folders, highlight_mode=True)
        create_libraries_zip(folders, now)


if __name__ == "__main__":
    main()

    # for root, dirs, files in os.walk('C:\\Users\\smarotta\\Desktop\\mirco_trasc_ann\\post_21-10-22\\word'):
    #     for f in tqdm(files):
    #         print("---------\n" + "WORKING ON: " + os.path.join(root, f))
    #         file_comments_dicts = return_comments_dicts(root, f)
    #         if "_annotato" in f:
    #             if file_comments_dicts:
    #                 print(root, f)
    #                 create_annotation_and_text_file(root, f, file_comments_dicts)
